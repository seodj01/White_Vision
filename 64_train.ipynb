{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 37607\n",
      "Validation dataset size: 8792\n",
      "=======================================================================\n",
      "Train image batch: torch.Size([64, 3, 256, 256])\n",
      "Train label shape: torch.Size([64, 256, 256])\n",
      "=======================================================================\n",
      "Validation imgage batch: torch.Size([64, 3, 256, 256])\n",
      "Validation label shape: torch.Size([64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class MultiFolderSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, folders, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.label_paths = []\n",
    "\n",
    "        # 폴더별 이미지 및 라벨 파일 경로 생성\n",
    "        for folder in folders:\n",
    "            tensor_folder = folder + \"_tensors_256x256(7 classes)\"\n",
    "            image_folder_path = os.path.join(root_dir, folder)\n",
    "            tensor_folder_path = os.path.join(root_dir, tensor_folder)\n",
    "\n",
    "            if not os.path.exists(tensor_folder_path):\n",
    "                raise ValueError(f\"Matching tensor folder not found for {folder}\")\n",
    "\n",
    "            for img_name in sorted(os.listdir(image_folder_path)):\n",
    "                if not img_name.endswith(\".jpg\"):\n",
    "                    continue  # JPG 파일만 처리\n",
    "\n",
    "                image_path = os.path.join(image_folder_path, img_name)\n",
    "                label_path = os.path.join(tensor_folder_path, os.path.splitext(img_name)[0] + \"_mask.pt\")\n",
    "\n",
    "                if not os.path.exists(label_path):\n",
    "                    raise ValueError(f\"Missing tensor file for {img_name} in {tensor_folder_path}\")\n",
    "\n",
    "                self.image_paths.append(image_path)\n",
    "                self.label_paths.append(label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 및 라벨 경로\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "\n",
    "        # 이미지와 라벨 데이터 로드\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = torch.load(label_path,weights_only=True)# .pt 텐서 로드 (256x256 크기)\n",
    "        label=label.to(torch.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# 상위 디렉토리 경로\n",
    "root_dir = \"/home/dongju/캡스톤디자인2\"\n",
    "\n",
    "# train과 val에 사용할 폴더 지정\n",
    "train_folders = [\"Surface_1\", \"Surface_2\", \"Surface_3\", \"Surface_4\"]\n",
    "val_folders = [\"Surface_5\"]\n",
    "\n",
    "# 이미지 변환 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet 통계\n",
    "])\n",
    "\n",
    "\n",
    "# train 데이터셋 및 데이터 로더\n",
    "train_dataset = MultiFolderSegmentationDataset(root_dir, train_folders, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# val 데이터셋 및 데이터 로더\n",
    "val_dataset = MultiFolderSegmentationDataset(root_dir, val_folders, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# 데이터 로더 확인\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(\"=======================================================================\")\n",
    "\n",
    "# 데이터 로더 사용 예시\n",
    "for images, labels in train_dataloader:\n",
    "    print(\"Train image batch:\", images.shape)\n",
    "    print(\"Train label shape:\",labels.shape)\n",
    "    print(\"=======================================================================\")\n",
    "    break\n",
    "\n",
    "for images, labels in val_dataloader:\n",
    "    print(\"Validation imgage batch:\", images.shape)\n",
    "    print(\"Validation label shape:\",labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongju/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dongju/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "\n",
    "# Squeeze-and-Excitation 모듈 정의\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        se = F.adaptive_avg_pool2d(x, 1).view(b, c)\n",
    "        se = F.relu(self.fc1(se))\n",
    "        se = torch.sigmoid(self.fc2(se)).view(b, c, 1, 1)\n",
    "        return x * se\n",
    "\n",
    "# Lightweight ASPP 모듈\n",
    "class LightweightASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(LightweightASPP, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=3, dilation=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=6, dilation=6)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(out_channels * 4, out_channels, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "        x3 = F.relu(self.conv3(x))\n",
    "\n",
    "        # Global context\n",
    "        x4 = self.global_avg_pool(x)\n",
    "        x4 = F.relu(self.global_conv(x4))\n",
    "        x4 = F.interpolate(x4, size=x1.size()[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        x = self.conv_out(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# MobileNetV3 기반 DeepLabV3 모델 정의\n",
    "class MobileNetV3DeepLabV3(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(MobileNetV3DeepLabV3, self).__init__()\n",
    "\n",
    "        # MobileNetV3 Large 백본 정의\n",
    "        mobilenet_v3_model = mobilenet_v3_large(pretrained=True)\n",
    "        self.backbone = mobilenet_v3_model.features\n",
    "\n",
    "        # 다운샘플링 비율을 줄이기 위해 마지막 단계의 stride 수정\n",
    "        # 마지막 레이어가 Conv2dNormActivation이므로 직접 접근하여 stride를 변경합니다.\n",
    "        for module in self.backbone[::-1]:\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                if module.kernel_size == (1, 1):\n",
    "                    module.stride = (1, 1)\n",
    "                    break\n",
    "\n",
    "        # ASPP 모듈\n",
    "        self.aspp = LightweightASPP(in_channels=960, out_channels=256)\n",
    "\n",
    "        # Squeeze-and-Excitation 모듈 추가\n",
    "        self.se = SqueezeExcitation(256)\n",
    "\n",
    "        # 최종 출력 레이어\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_size = x.size()[2:]\n",
    "\n",
    "        # MobileNetV3 백본을 사용하여 특징 추출\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # ASPP 모듈 적용\n",
    "        x = self.aspp(x)\n",
    "\n",
    "        # Squeeze-and-Excitation 모듈 적용\n",
    "        x = self.se(x)\n",
    "\n",
    "        # 클래스 점수 계산\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # 원본 이미지 크기로 업샘플링\n",
    "        x = F.interpolate(x, size=x_size, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "# 모델 생성 및 장치 설정\n",
    "num_classes = 8  # 배경을 포함한 총 8개 클래스\n",
    "model = MobileNetV3DeepLabV3(num_classes=num_classes)\n",
    "\n",
    "# 모델을 GPU 또는 CPU에 할당\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 클래스 이름 (배경 제외)\n",
    "CLASS_NAMES_NO_BG = [\"Sidewalk\", \"Braille Guide Blocks\", \"Roadway\", \"Alley\", \"Bike Lane\", \"Caution Zone\", \"Cross walk\"]\n",
    "\n",
    "# 배치별 메트릭을 CSV로 저장하는 함수 (배경 제외)\n",
    "def save_batch_metrics_to_csv(epoch, phase, batch_loss, batch_iou, pixel_acc, class_iou, filename=\"batch_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    배치별 메트릭을 하나의 CSV 파일에 저장\n",
    "    :param epoch: 현재 에폭\n",
    "    :param phase: \"train\" 또는 \"val\"로 단계 표시\n",
    "    :param batch_loss: 배치 Loss\n",
    "    :param batch_iou: 배치 IoU\n",
    "    :param pixel_acc: 픽셀 정확도\n",
    "    :param class_iou: 클래스별 IoU 리스트\n",
    "    :param filename: 저장할 CSV 파일 이름\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_exists = os.path.isfile(filename)\n",
    "        with open(filename, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # 파일이 없으면 헤더 추가\n",
    "            if not file_exists:\n",
    "                header = [\"epoch\", \"phase\", \"batch_loss\", \"batch_iou\", \"pixel_acc\"] + [f\"iou_{name}\" for name in CLASS_NAMES_NO_BG]\n",
    "                writer.writerow(header)\n",
    "            # 배치 데이터 저장\n",
    "            row = [epoch, phase, batch_loss, batch_iou, pixel_acc] + (class_iou.tolist() if phase == \"val\" else [\"\"] * len(CLASS_NAMES_NO_BG))\n",
    "            writer.writerow(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save batch metrics to CSV: {e}\")\n",
    "\n",
    "# 에폭 단위 메트릭을 CSV로 저장하는 함수 (배경 제외)\n",
    "def save_epoch_metrics_to_csv(epoch, phase, epoch_loss, epoch_iou, pixel_acc, class_iou, filename=\"epoch_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    에폭 단위 메트릭을 하나의 CSV 파일에 저장\n",
    "    :param epoch: 현재 에폭\n",
    "    :param phase: \"train\" 또는 \"val\"로 단계 표시\n",
    "    :param epoch_loss: 에폭 평균 Loss\n",
    "    :param epoch_iou: 에폭 평균 IoU\n",
    "    :param pixel_acc: 픽셀 정확도\n",
    "    :param class_iou: 클래스별 IoU 리스트\n",
    "    :param filename: 저장할 CSV 파일 이름\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_exists = os.path.isfile(filename)\n",
    "        with open(filename, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # 파일이 없으면 헤더 추가\n",
    "            if not file_exists:\n",
    "                header = [\"epoch\", \"phase\", \"epoch_loss\", \"epoch_iou\", \"pixel_acc\"] + [f\"iou_{name}\" for name in CLASS_NAMES_NO_BG]\n",
    "                writer.writerow(header)\n",
    "            # 에폭 데이터 저장\n",
    "            row = [epoch, phase, epoch_loss, epoch_iou, pixel_acc] + class_iou.tolist()\n",
    "            writer.writerow(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save epoch metrics to CSV: {e}\")\n",
    "\n",
    "# 클래스별 IoU 계산 (배경 제외 가능)\n",
    "def calculate_iou_per_class(pred, target, num_classes, exclude_background=True):\n",
    "    iou_per_class = torch.zeros(num_classes, dtype=torch.float32)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        union = (pred_inds | target_inds).sum().float()\n",
    "        if union == 0:\n",
    "            iou_per_class[cls] = 0.0\n",
    "        else:\n",
    "            iou_per_class[cls] = intersection / union\n",
    "\n",
    "    # 배경을 제외하고 나머지 클래스 IoU만 반환\n",
    "    if exclude_background:\n",
    "        return iou_per_class[1:]\n",
    "\n",
    "    return iou_per_class\n",
    "\n",
    "# 픽셀 정확도 계산\n",
    "def calculate_pixel_accuracy(pred, target):\n",
    "    correct = (pred == target).sum().item()\n",
    "    total = target.numel()\n",
    "    return correct / total\n",
    "\n",
    "# 모델 학습 및 검증 함수\n",
    "def train_model_with_csv_and_checkpoint(\n",
    "    model, train_dataloader, val_dataloader, criterion, optimizer, \n",
    "    num_classes, num_epochs=25, device='cuda:0', csv_filename=\"metrics.csv\", \n",
    "    checkpoint_dir=\"./checkpoints\", checkpoint_file=None, batch_csv_filename=\"batch_metrics.csv\", \n",
    "    epoch_csv_filename=\"epoch_metrics.csv\"\n",
    "):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model.to(device)\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    # 체크포인트 로드\n",
    "    if checkpoint_file and os.path.isfile(checkpoint_file):\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1}.\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting training from scratch.\")\n",
    "    \n",
    "    # 학습 루프\n",
    "    for epoch in range(start_epoch + 1, start_epoch + num_epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{start_epoch + num_epochs}\")\n",
    "\n",
    "        # 학습 단계\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_iou = []\n",
    "        train_pixel_acc = []\n",
    "\n",
    "        train_loader = tqdm(train_dataloader, desc=f\"Train Epoch {epoch}\", leave=False)\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images, masks = images.to(device), masks.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            batch_loss = loss.item()\n",
    "            batch_iou = calculate_iou_per_class(outputs, masks, num_classes, exclude_background=True).mean().item()\n",
    "            pixel_acc = calculate_pixel_accuracy(outputs, masks)\n",
    "\n",
    "            train_loss.append(batch_loss)\n",
    "            train_iou.append(batch_iou)\n",
    "            train_pixel_acc.append(pixel_acc)\n",
    "\n",
    "            # 배치 데이터를 CSV에 저장\n",
    "            save_batch_metrics_to_csv(epoch, \"train\", batch_loss, batch_iou, pixel_acc, [], filename=batch_csv_filename)\n",
    "\n",
    "        # 에폭 평균 계산\n",
    "        epoch_train_loss = np.mean(train_loss)\n",
    "        epoch_train_iou = np.mean(train_iou)\n",
    "        epoch_train_pixel_acc = np.mean(train_pixel_acc)\n",
    "\n",
    "        # 에폭 데이터를 CSV에 저장\n",
    "        save_epoch_metrics_to_csv(epoch, \"train\", epoch_train_loss, epoch_train_iou, epoch_train_pixel_acc, torch.zeros(num_classes - 1), filename=epoch_csv_filename)\n",
    "\n",
    "        # 검증 단계\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        val_iou = []\n",
    "        val_pixel_acc = []\n",
    "        class_iou = torch.zeros(num_classes - 1)  # 배경 제외한 클래스 개수로 초기화\n",
    "\n",
    "        val_loader = tqdm(val_dataloader, desc=\"Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, masks) in enumerate(val_loader):\n",
    "                images, masks = images.to(device), masks.long().to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                batch_loss = loss.item()\n",
    "                class_iou_batch = calculate_iou_per_class(outputs, masks, num_classes, exclude_background=True)\n",
    "                batch_iou = class_iou_batch.mean().item()\n",
    "                pixel_acc = calculate_pixel_accuracy(outputs, masks)\n",
    "\n",
    "                val_loss.append(batch_loss)\n",
    "                val_iou.append(batch_iou)\n",
    "                val_pixel_acc.append(pixel_acc)\n",
    "                class_iou += class_iou_batch\n",
    "\n",
    "                # 배치 데이터를 CSV에 저장\n",
    "                save_batch_metrics_to_csv(epoch, \"val\", batch_loss, batch_iou, pixel_acc, class_iou_batch, filename=batch_csv_filename)\n",
    "\n",
    "        # 에폭 평균 계산\n",
    "        epoch_val_loss = np.mean(val_loss)\n",
    "        epoch_val_iou = np.mean(val_iou)\n",
    "        epoch_val_pixel_acc = np.mean(val_pixel_acc)\n",
    "        class_iou /= len(val_loader)  # 평균 IoU 계산\n",
    "\n",
    "        # 에폭 데이터를 CSV에 저장\n",
    "        save_epoch_metrics_to_csv(epoch, \"val\", epoch_val_loss, epoch_val_iou, epoch_val_pixel_acc, class_iou, filename=epoch_csv_filename)\n",
    "\n",
    "        # 에폭 종료 시 모델 체크포인트 저장\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        # 최종 결과 출력 (배경 제외)\n",
    "        print(\"\\n========================= Epoch Results =========================\")\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train mIoU: {epoch_train_iou:.4f}\")\n",
    "        print(f\"Val Loss:   {epoch_val_loss:.4f}, Val mIoU:   {epoch_val_iou:.4f}\")\n",
    "        print(f\"Class-wise Val IoU:\")\n",
    "        for idx, iou in enumerate(class_iou.tolist()):\n",
    "            print(f\"  {CLASS_NAMES_NO_BG[idx]}: {iou:.4f}\")\n",
    "        print(\"================================================================\")\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "No checkpoint found. Starting training from scratch.\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_1.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.9148, Train mIoU: 0.2888\n",
      "Val Loss:   0.6210, Val mIoU:   0.3638\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7127\n",
      "  Braille Guide Blocks: 0.1392\n",
      "  Roadway: 0.2024\n",
      "  Alley: 0.7349\n",
      "  Bike Lane: 0.1340\n",
      "  Caution Zone: 0.4755\n",
      "  Cross walk: 0.1483\n",
      "================================================================\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_2.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.5280, Train mIoU: 0.3838\n",
      "Val Loss:   0.4733, Val mIoU:   0.4111\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7253\n",
      "  Braille Guide Blocks: 0.1730\n",
      "  Roadway: 0.2559\n",
      "  Alley: 0.7496\n",
      "  Bike Lane: 0.1708\n",
      "  Caution Zone: 0.5416\n",
      "  Cross walk: 0.2613\n",
      "================================================================\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_3.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.4322, Train mIoU: 0.4153\n",
      "Val Loss:   0.4057, Val mIoU:   0.4341\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7305\n",
      "  Braille Guide Blocks: 0.2257\n",
      "  Roadway: 0.2700\n",
      "  Alley: 0.7534\n",
      "  Bike Lane: 0.1853\n",
      "  Caution Zone: 0.5610\n",
      "  Cross walk: 0.3129\n",
      "================================================================\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_4.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.3776, Train mIoU: 0.4328\n",
      "Val Loss:   0.3805, Val mIoU:   0.4521\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7333\n",
      "  Braille Guide Blocks: 0.2737\n",
      "  Roadway: 0.2768\n",
      "  Alley: 0.7537\n",
      "  Bike Lane: 0.1875\n",
      "  Caution Zone: 0.5901\n",
      "  Cross walk: 0.3497\n",
      "================================================================\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_5.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.3389, Train mIoU: 0.4525\n",
      "Val Loss:   0.3588, Val mIoU:   0.4639\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7318\n",
      "  Braille Guide Blocks: 0.3084\n",
      "  Roadway: 0.2892\n",
      "  Alley: 0.7615\n",
      "  Bike Lane: 0.1901\n",
      "  Caution Zone: 0.5997\n",
      "  Cross walk: 0.3666\n",
      "================================================================\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_6.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.3093, Train mIoU: 0.4661\n",
      "Val Loss:   0.3474, Val mIoU:   0.4773\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7296\n",
      "  Braille Guide Blocks: 0.3693\n",
      "  Roadway: 0.2876\n",
      "  Alley: 0.7572\n",
      "  Bike Lane: 0.2020\n",
      "  Caution Zone: 0.6078\n",
      "  Cross walk: 0.3873\n",
      "================================================================\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_7.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.2821, Train mIoU: 0.4828\n",
      "Val Loss:   0.3416, Val mIoU:   0.4810\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7305\n",
      "  Braille Guide Blocks: 0.3854\n",
      "  Roadway: 0.2808\n",
      "  Alley: 0.7611\n",
      "  Bike Lane: 0.2072\n",
      "  Caution Zone: 0.5996\n",
      "  Cross walk: 0.4025\n",
      "================================================================\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8:  71%|███████▏  | 420/588 [05:13<01:47,  1.56it/s]"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CUDA 환경 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "num_classes = 8 # 실제 학습할 클래스 수 (\n",
    "learning_rate = 0.00001\n",
    "weight_decay = 0.00001\n",
    "num_epochs = 10\n",
    "\n",
    "# 모델, 옵티마이저, 손실 함수 설정\n",
    "model = MobileNetV3DeepLabV3(num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# 손실 함수 및 장치 설정 (가중치 없이)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "\n",
    "# 기존 학습 코드 실행 (CSV 파일 두 개 지정)\n",
    "train_model_with_csv_and_checkpoint(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    batch_csv_filename=\"./64_batch_metrics.csv\",  # 배치 단위 메트릭\n",
    "    epoch_csv_filename=\"./64_epoch_metrics.csv\",  # 에폭 단위 메트릭\n",
    "    checkpoint_dir=\"./64_batch_metrics_checkpoints\",\n",
    "    checkpoint_file=None\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongju/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dongju/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_3380208/2606396079.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Resuming training from epoch 8.\n",
      "\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_8.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.2573, Train mIoU: 0.4949\n",
      "Val Loss:   0.3486, Val mIoU:   0.4874\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7200\n",
      "  Braille Guide Blocks: 0.4135\n",
      "  Roadway: 0.2924\n",
      "  Alley: 0.7646\n",
      "  Bike Lane: 0.2187\n",
      "  Caution Zone: 0.6088\n",
      "  Cross walk: 0.3935\n",
      "================================================================\n",
      "\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_9.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.2344, Train mIoU: 0.5102\n",
      "Val Loss:   0.3552, Val mIoU:   0.4900\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7164\n",
      "  Braille Guide Blocks: 0.4301\n",
      "  Roadway: 0.3008\n",
      "  Alley: 0.7613\n",
      "  Bike Lane: 0.2186\n",
      "  Caution Zone: 0.6066\n",
      "  Cross walk: 0.3961\n",
      "================================================================\n",
      "\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_10.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.2133, Train mIoU: 0.5252\n",
      "Val Loss:   0.3574, Val mIoU:   0.4936\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7238\n",
      "  Braille Guide Blocks: 0.4629\n",
      "  Roadway: 0.2873\n",
      "  Alley: 0.7611\n",
      "  Bike Lane: 0.2270\n",
      "  Caution Zone: 0.6041\n",
      "  Cross walk: 0.3894\n",
      "================================================================\n",
      "\n",
      "Epoch 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 기존 학습 코드 실행 (CSV 파일 두 개 지정)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain_model_with_csv_and_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_csv_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./64_batch_metrics.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 배치 단위 메트릭\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_csv_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./64_epoch_metrics.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 에폭 단위 메트릭\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./64_batch_metrics_checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./64_batch_metrics_checkpoints/model_epoch_7.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 121\u001b[0m, in \u001b[0;36mtrain_model_with_csv_and_checkpoint\u001b[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, num_classes, num_epochs, device, csv_filename, checkpoint_dir, checkpoint_file, batch_csv_filename, epoch_csv_filename)\u001b[0m\n\u001b[1;32m    118\u001b[0m train_pixel_acc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    120\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    122\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), masks\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    124\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CUDA 환경 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "num_classes = 8 # 실제 학습할 클래스 수 (\n",
    "learning_rate = 0.000005\n",
    "weight_decay = 0.000005\n",
    "num_epochs = 5\n",
    "\n",
    "# 모델, 옵티마이저, 손실 함수 설정\n",
    "model = MobileNetV3DeepLabV3(num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# 손실 함수 및 장치 설정 (가중치 없이)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "\n",
    "# 기존 학습 코드 실행 (CSV 파일 두 개 지정)\n",
    "train_model_with_csv_and_checkpoint(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    batch_csv_filename=\"./64_batch_metrics.csv\",  # 배치 단위 메트릭\n",
    "    epoch_csv_filename=\"./64_epoch_metrics.csv\",  # 에폭 단위 메트릭\n",
    "    checkpoint_dir=\"./64_batch_metrics_checkpoints\",\n",
    "    checkpoint_file=\"./64_batch_metrics_checkpoints/model_epoch_7.pth\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3630478/2606396079.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Resuming training from epoch 11.\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_11.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1931, Train mIoU: 0.5386\n",
      "Val Loss:   0.3652, Val mIoU:   0.4976\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7241\n",
      "  Braille Guide Blocks: 0.4829\n",
      "  Roadway: 0.2774\n",
      "  Alley: 0.7618\n",
      "  Bike Lane: 0.2242\n",
      "  Caution Zone: 0.6055\n",
      "  Cross walk: 0.4071\n",
      "================================================================\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_12.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1777, Train mIoU: 0.5512\n",
      "Val Loss:   0.3756, Val mIoU:   0.5016\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7174\n",
      "  Braille Guide Blocks: 0.4948\n",
      "  Roadway: 0.2889\n",
      "  Alley: 0.7649\n",
      "  Bike Lane: 0.2251\n",
      "  Caution Zone: 0.6116\n",
      "  Cross walk: 0.4084\n",
      "================================================================\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_13.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1613, Train mIoU: 0.5645\n",
      "Val Loss:   0.3816, Val mIoU:   0.5017\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7190\n",
      "  Braille Guide Blocks: 0.5203\n",
      "  Roadway: 0.2771\n",
      "  Alley: 0.7536\n",
      "  Bike Lane: 0.2373\n",
      "  Caution Zone: 0.6094\n",
      "  Cross walk: 0.3951\n",
      "================================================================\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_14.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1499, Train mIoU: 0.5719\n",
      "Val Loss:   0.3944, Val mIoU:   0.5088\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7097\n",
      "  Braille Guide Blocks: 0.5373\n",
      "  Roadway: 0.2963\n",
      "  Alley: 0.7572\n",
      "  Bike Lane: 0.2525\n",
      "  Caution Zone: 0.6135\n",
      "  Cross walk: 0.3952\n",
      "================================================================\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_15.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1389, Train mIoU: 0.5832\n",
      "Val Loss:   0.3993, Val mIoU:   0.5056\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7150\n",
      "  Braille Guide Blocks: 0.5494\n",
      "  Roadway: 0.2736\n",
      "  Alley: 0.7568\n",
      "  Bike Lane: 0.2437\n",
      "  Caution Zone: 0.6065\n",
      "  Cross walk: 0.3947\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CUDA 환경 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "num_classes = 8 # 실제 학습할 클래스 수 (\n",
    "learning_rate = 0.000001\n",
    "weight_decay = 0.000001\n",
    "num_epochs = 5\n",
    "\n",
    "# 모델, 옵티마이저, 손실 함수 설정\n",
    "model = MobileNetV3DeepLabV3(num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# 손실 함수 및 장치 설정 (가중치 없이)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "\n",
    "# 기존 학습 코드 실행 (CSV 파일 두 개 지정)\n",
    "train_model_with_csv_and_checkpoint(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    batch_csv_filename=\"./64_batch_metrics.csv\",  # 배치 단위 메트릭\n",
    "    epoch_csv_filename=\"./64_epoch_metrics.csv\",  # 에폭 단위 메트릭\n",
    "    checkpoint_dir=\"./64_batch_metrics_checkpoints\",\n",
    "    checkpoint_file=\"./64_batch_metrics_checkpoints/model_epoch_10.pth\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3630478/2606396079.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Resuming training from epoch 14.\n",
      "\n",
      "Epoch 14/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_14.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1506, Train mIoU: 0.5728\n",
      "Val Loss:   0.3907, Val mIoU:   0.5058\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7183\n",
      "  Braille Guide Blocks: 0.5357\n",
      "  Roadway: 0.2869\n",
      "  Alley: 0.7555\n",
      "  Bike Lane: 0.2379\n",
      "  Caution Zone: 0.6071\n",
      "  Cross walk: 0.3995\n",
      "================================================================\n",
      "\n",
      "Epoch 15/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_15.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1387, Train mIoU: 0.5841\n",
      "Val Loss:   0.3987, Val mIoU:   0.5089\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7145\n",
      "  Braille Guide Blocks: 0.5539\n",
      "  Roadway: 0.2785\n",
      "  Alley: 0.7556\n",
      "  Bike Lane: 0.2438\n",
      "  Caution Zone: 0.6143\n",
      "  Cross walk: 0.4013\n",
      "================================================================\n",
      "\n",
      "Epoch 16/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_16.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1295, Train mIoU: 0.5906\n",
      "Val Loss:   0.4082, Val mIoU:   0.5111\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7171\n",
      "  Braille Guide Blocks: 0.5672\n",
      "  Roadway: 0.2817\n",
      "  Alley: 0.7586\n",
      "  Bike Lane: 0.2400\n",
      "  Caution Zone: 0.6164\n",
      "  Cross walk: 0.3968\n",
      "================================================================\n",
      "\n",
      "Epoch 17/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_17.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1211, Train mIoU: 0.5986\n",
      "Val Loss:   0.4259, Val mIoU:   0.5095\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7176\n",
      "  Braille Guide Blocks: 0.5751\n",
      "  Roadway: 0.2809\n",
      "  Alley: 0.7440\n",
      "  Bike Lane: 0.2407\n",
      "  Caution Zone: 0.6108\n",
      "  Cross walk: 0.3977\n",
      "================================================================\n",
      "\n",
      "Epoch 18/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_18.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1148, Train mIoU: 0.6041\n",
      "Val Loss:   0.4254, Val mIoU:   0.5125\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7143\n",
      "  Braille Guide Blocks: 0.5795\n",
      "  Roadway: 0.2834\n",
      "  Alley: 0.7540\n",
      "  Bike Lane: 0.2424\n",
      "  Caution Zone: 0.6161\n",
      "  Cross walk: 0.3981\n",
      "================================================================\n",
      "\n",
      "Epoch 19/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_19.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1074, Train mIoU: 0.6108\n",
      "Val Loss:   0.4286, Val mIoU:   0.5153\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7128\n",
      "  Braille Guide Blocks: 0.5840\n",
      "  Roadway: 0.2930\n",
      "  Alley: 0.7529\n",
      "  Bike Lane: 0.2481\n",
      "  Caution Zone: 0.6171\n",
      "  Cross walk: 0.3990\n",
      "================================================================\n",
      "\n",
      "Epoch 20/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_20.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.1029, Train mIoU: 0.6160\n",
      "Val Loss:   0.4467, Val mIoU:   0.5112\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7205\n",
      "  Braille Guide Blocks: 0.5926\n",
      "  Roadway: 0.2689\n",
      "  Alley: 0.7522\n",
      "  Bike Lane: 0.2432\n",
      "  Caution Zone: 0.6114\n",
      "  Cross walk: 0.3895\n",
      "================================================================\n",
      "\n",
      "Epoch 21/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_21.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.0974, Train mIoU: 0.6176\n",
      "Val Loss:   0.4472, Val mIoU:   0.5161\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7174\n",
      "  Braille Guide Blocks: 0.5990\n",
      "  Roadway: 0.2848\n",
      "  Alley: 0.7557\n",
      "  Bike Lane: 0.2402\n",
      "  Caution Zone: 0.6126\n",
      "  Cross walk: 0.4029\n",
      "================================================================\n",
      "\n",
      "Epoch 22/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_22.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.0937, Train mIoU: 0.6248\n",
      "Val Loss:   0.4624, Val mIoU:   0.5130\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7150\n",
      "  Braille Guide Blocks: 0.6011\n",
      "  Roadway: 0.2794\n",
      "  Alley: 0.7535\n",
      "  Bike Lane: 0.2411\n",
      "  Caution Zone: 0.6114\n",
      "  Cross walk: 0.3897\n",
      "================================================================\n",
      "\n",
      "Epoch 23/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./64_batch_metrics_checkpoints/model_epoch_23.pth\n",
      "\n",
      "========================= Epoch Results =========================\n",
      "Train Loss: 0.0889, Train mIoU: 0.6275\n",
      "Val Loss:   0.4562, Val mIoU:   0.5171\n",
      "Class-wise Val IoU:\n",
      "  Sidewalk: 0.7106\n",
      "  Braille Guide Blocks: 0.5926\n",
      "  Roadway: 0.2851\n",
      "  Alley: 0.7556\n",
      "  Bike Lane: 0.2514\n",
      "  Caution Zone: 0.6204\n",
      "  Cross walk: 0.4036\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CUDA 환경 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "num_classes = 8 # 실제 학습할 클래스 수 (\n",
    "learning_rate = 0.00001\n",
    "weight_decay = 0.00001\n",
    "num_epochs = 10\n",
    "\n",
    "# 모델, 옵티마이저, 손실 함수 설정\n",
    "model = MobileNetV3DeepLabV3(num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# 손실 함수 및 장치 설정 (가중치 없이)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "\n",
    "# 기존 학습 코드 실행 (CSV 파일 두 개 지정)\n",
    "train_model_with_csv_and_checkpoint(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    batch_csv_filename=\"./64_batch_metrics.csv\",  # 배치 단위 메트릭\n",
    "    epoch_csv_filename=\"./64_epoch_metrics.csv\",  # 에폭 단위 메트릭\n",
    "    checkpoint_dir=\"./64_batch_metrics_checkpoints\",\n",
    "    checkpoint_file=\"./64_batch_metrics_checkpoints/model_epoch_13.pth\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 12꺼가 제일 베스트!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
